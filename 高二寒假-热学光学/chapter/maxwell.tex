%!TEX root = ../xesphVI.tex
\chapter{统计物理基础摘要}

\section{数学基础}

\begin{comment}
\subsection{概率与独立性}
现实生活中有很多偶然事件,\,偶然事件的成因是多种多样的,\,它们集中表现在相似的条件下进行试验,\,而能够得到完全不同的结果.\,数学上用抽象的集合来表示所有可能的结果:
\[x\in X,\quad X=\{x\}\]

其中每一个元素代表一种可能的结果.\,这些结果应该有以下特点:
\begin{quote}
{\hei 忠实性}:\,每一个不同的实验结果如实地反应为集合中的不同元素.\,也就是不能用一个元素代替一类实验结果.\\
{\hei 互斥性}:\,当一个结果发生时,\,另一个结果就必须排除,\,也就是不能有多个元素对应同样的实验结果.\\
{\hei 完备性}:\,所有的实验结果必须都有集合中的元素对应.
\end{quote}

这样就能把集合\(X\)称为\emph{样本空间}(sample space),\,而每一个元素\(x\)也称为一个\emph{元事件}(elementary event).\,我们常说的\emph{事件}(event),\,其实一般指样本空间的一些定义良好的子集\(A\subset X\).\,只要实验结果在这个子集中,\,就说这个事件发生了.\,所谓定义良好我们可以做如下理解:

在\emph{古典概型}(classic models)情形下,\,样本空间是一个有限的集合,\,此时任意子集都可以视为某种事件.\,如投一颗骰子,\,样本空间为\(X=\{1,2,3,4,5,6\}\),\,则投出偶数是一个事件\(A=\{2,4,6\}\).

但在\emph{几何概型}(geometric models)情形下,\,样本空间一般具有与\(\mathbb{R}^n\)类似的结构,\,一般都是无限的集合,\,有一些``事件''的提法应当给予摒弃否则会引起矛盾.\,例如在闭区间\([0,1]\)间任取一个点,\,这个点恰好是有理数这样的``事件''可能就不是那么定义良好\footnote{事实上有理数集是定义良好---可测的,\,但有些集合不可测从而不能讨论它们的概率.\,参考\url{https://en.wikipedia.org/wiki/Non-measurable_set}}.\,一般常见的事件如点落在区间\((a,b)\)内等等.

设想同时做好几个实验,\,这几个实验互不干扰\,它们的结果是完全独立的,\,那么联合到一起就构成了一个大的实验,\,其结果应表示为一个数组:
\[\bs{x}=(x_i)\quad ;\quad x_i\in X_i\]

而新的样本空间称为原来那些样本空间的\emph{独立直积}(independent product).\,记做:
\[\bigotimes_i X_i=\{\bs{x}=(x_{ij})|x_{ij}\in X_i\}\]

互斥,\,独立这样的一些概念如何用数学严格表述?\,事实上它们恰恰是用概率去定义的.\,概率是一种我们关于实验结果可能性的\emph{先验假设}(a priori presumption),\,它是\emph{随机变量}(random variable)的正实数值函数\(P\),\,而随机变量既可以取为样本空间中的单个结果,\,也可以取为结果的集合:\,事件:
\[P(A)=P(\bigcup_{x_i\in A} {x_i})=\sum_{x_i\in A} P(x_i)\]

这其中已用到了\(P\)的属性:\,\emph{互斥事件}(mutual-exclusive events, disjoint events)的加法原理:
\[A\cap B=\emptyset\quad \Rightarrow\quad P(A\cup B)=P(A)+P(B)\]

再加上:
\[P(A)\geqslant 0\quad ;\quad P(X)=1\]

就构成了一个合适的概率定义.\,而对于随机变量可取连续样本空间,\,比如区间\([a,b]\)的情形,\,引入\emph{概率密度函数}(probability density function, pdf):
\[P([x,x+\ud x])=p(x)\ud x\quad ;\quad p(x)>0\;,\;\int_a^b p(x)\ud x=1\]

便是一个合适的概率定义.

现在来讨论事件的独立性.\,两个非互斥的事件可能同时发生,\,同时发生这一个新的事件即被定义为\(A\cap B\neq \emptyset\),\,此时可以定义事件\(B\)在事件\(A\)下的\emph{条件概率}(conditional probability):
\[P(B|A)=\frac{P(A\cap B)}{P(A)}\]

显然,\,若条件概率为零,\,那么实际上两个事件互斥.\,而实际上如果\(B\)在\(A\)下的条件概率等于\(B\)的概率,\,那么这种情况称为两个事件相互独立:
\[P(B|A)=P(B)\]

注意到上式实际上也可以写为:
\[P(X)P(A\cap B)=P(A)P(B)\]

所以两个事件相互独立的确是一个相互的关系,\,此时\(A\)的条件概率也有\(P(A|B)=P(A)\).\,两个事件相互独立是一种很微妙的关系,\,这意味着一个事件的发生既不会阻碍另一个事件,\,也不会促成另一个事件.\,而是完全没有影响.\,物理上看,\,很有可能两个事件没有因果关系.

我们之前构造的独立直积,\,用概率表示即为:
\[P((x_{ij}))=\prod_i P(x_{ij})\]

而对于连续概率分布,\,两个样本空间的独立直积给出的新概率应该是有以下概率密度:
\[p(x,y)=p_1(x)p_2(y)\]

\subsection{随机变量及其数字特征}
物理实验中的随机现象,\,常常体现为实验测量中测量结果数据的上下波动.\,此时一般认为这个测量数据\(x\)就是一个随机变量,\,而具有先验的某种概率密度函数\(p(x)\).\,.\,有时候我们关心测量数据的某种函数\(y=f(x)\),\,那么不同的\(y\)其出现概率分布应该修改:
\[p_y(y)=\frac{\ud p}{\ud y}=\frac{p(x)\ud x}{\ud y}=\frac{1}{f'}p(x)\]

例如,\,如果考虑在\(x\)方向的粒子速度\(v\)具有分布律:
\[p(v)=\frac{1}{\sqrt{2\pi}}\ue^{-\frac{v^2}{2}}\]

可以验证这个概率归一,\,那么其能量\(e=\dfrac{v^2}{2}\)的分布为
\[p_{e+}=\frac{p(v)}{v}=\frac{1}{2\sqrt{\pi e}}\ue^{-e}(v>0)\quad ;\quad p_{e-}=\frac{p(v)}{-v}=\frac{1}{2\sqrt{\pi e}}\ue^{-e}(v<0)\]

注意到同一个能量对应着两种可能的运动方向,\,故把两个\(\ud e\)对应概率分开算,\,最后合到一起:
\[p_e(e)=\frac{1}{\sqrt{\pi e}}\ue^{-e}\]

一个在统计中关心的数学处理,\,是在\(n\)次全同独立测量中获得的测量结果的\emph{平均数}(average):
\[\bar{x}=\frac{x_1+x_2+\cdots+x_n}{n}\]

而著名的\emph{大数定律}(law of large numbers)指出:\,如果概率分布的一个重要数字特征:\,\emph{期望}(expectation)存在\footnote{值得指出,\,有些数学上诡异的概率分布是不存在期望的,\,比如典型的柯西分布:
\[p(x)=\frac{\gamma/\pi}{x^2+\gamma^2}\]

当然这样的理想概率分布物理上几乎不存在,\,因为物理量总是有限的,\,在一定尺度下适用.}:
\[\mu=\langle x\rangle=\int x\ud p\]

那么如果随着测量次数趋于无穷,\,平均数将会无限趋近于期望值:
\[\lim_{n\to \infty}P(|\bar{x}-\langle x\rangle|>\varepsilon)=0\; , \; \forall \varepsilon>0\]

如何理解这一点?\,我们再定义另一个随机变量的数字特征:\,\emph{方差}(variance),\,物理上称为\emph{涨落}(fluctuation),\,数学上还称\emph{中心二极矩}(second central moment):
\[\sigma^2=\langle(x-\mu)^2\rangle=\int(x-\int x\ud p)^2\ud p\]

物理上还常常把涨落除以期望的平方称为相对涨落.\,那么可以证明:
\[\langle\bar{x}\rangle=\langle x\rangle\quad ;\quad \langle(\bar{x}-\mu)^2\rangle=\frac{\langle(x-\mu)^2\rangle}{n}\]

故随着实验次数增加,\,平均数作为新的随机变量它的期望不会变,\,而方差在不断减小,\,故有中心极限定理的说法.

以上讨论适用于任何\(y=f(x)\)型的随机变量,\,其中\(x\)可以是一次实验的实数结果,\,也可以是多次实验的数组结果,\,也可以是不同独立甚至非独立测量结果的复合,\,此时\(y\)即被理解为事件,\,代表某些可能发生的元事件的集合.\,期望与方差被定义为:
\[\mu=\int y\ud p\]
\[\sigma^2=\int (y-\mu)^2\ud p\]

其中\(\ud p\)表示\(y\)落在该点附近范围内的概率.

\subsection{信息熵}
为了刻画样本空间与其概率的多样性,\,我们引入\emph{信息熵}(information entropy)的概念.\,对于一个样本空间与随机分布,\,它具有的熵被定义为:
\[S=\sum_{x_i\in X}-P(x_i)\ln P(x_i)\]

而每一种可能性\(x_i\)带来的信息量被定义为\(I_i=-\ln P(x_i)>0\).\,而总熵是事件信息量的期望.\,为什么如此定义?\,比如我们设想投一枚骰子,\,那么投出\(1\)这个数的信息量应该是投出奇数的信息量加上在已知投出奇数后再投出\(1\)的信息量,\,投出奇数称作\(A\)事件那么这个性质写作:
\[I(x)=I(A)+I(x|A)\]

而应该有一种概率与信息量之间的普适对应,\,以上三个信息都可以理解为:
\[I(x)=f(P(x)),\,I(A)=f(P(A)),\,I(x|A)=f(P(x|A))\]

但三个概率本来就有:
\[P(x)=P(A)P(x|A)\]

这暗示着\(f(xy)=f(x)+f(y)\),\,在假定\(f\)性质足够好(可以求二阶导数)情况下:
\[\xrightarrow{\;\partial_y\;} xf'(xy)=f'(y)\]
\[\xrightarrow{\;\partial_x\;} f''(xy)+xyf'(xy)=0\]
\[\xrightarrow{xy=u\;} \frac{\ud}{\ud u}(u\frac{\ud f}{\ud u})=0\]
\[\Rightarrow f=C\ln u+C'\]

重新代入\(f\)性质,\,得\(C'=0\).\,剩下的\(C\)可以任取,\,一般理解为\(-1\),\,这样给出了正的信息量.\,这个推理过程还给出了事件的信息量\(I(A)\)与条件信息量\(I(A|B)\)两个概念的定义.\,根据定义,\,可见\(A\)取整个样本空间的信息量就是零.

现在我们理解到信息量表示稀有程度,\,具体事件发生概率越小则信息量越大.\,那熵又表示什么含义呢.\,它表示多样性.\,我们设想如果某一种元事件\(x_i\)的可能性被发现还可以细分为\(x_{i1},\,x_{i2},\cdots,x_{im}\)种子可能性.\,那么根据此式所引入的熵就会因此而增加,\,增量为:
\begin{align*}
\Delta S & = \sum_{j=1}^m -P(x_{ij})\ln P(x_{ij})-(-P(x_i)\ln P(x_i))\\
		 & = \sum_{j=1}^m -P(x_{ij})\ln P(x_{ij})-\sum_{j=1}^m -P(x_{ij})\ln P(x_i)\\
		 & = \sum_{j=1}^m -P(x_{ij})\ln\frac{P(x_{ij})}{P(x_i)}\\
		 & = \sum_{j=1}^m P(x_{ij})I(x_{ij}|x_i)\\
		 & = P(x_i)\cdot \sum_{j=1}^m -\frac{P(x_{ij})}{P(x_i)}\ln\frac{P(x_{ij})}{P(x_i)}\\
		 & = P(x_i)\cdot \sum_{j=1}^m -P(x_{ij}|x_i)\ln P(x_{ij}|x_i)\\
		 & = P(x_i) S(x_i)
\end{align*}

一方面,\,等于细化这一元事件所增加的信息量的概率平均.\,另一方面,\,也等于事件熵与概率的乘积.\,此处也定义了事件细分熵:
\[S(A)=\sum_{x_i\in A}-P(x_i|A)\ln P(x_i|A)\]

可见单个元事件的细分熵是零.\,而如果把样本空间可以分为一系列不相交的事件:
\[X=\bigcup_i A_i\quad ;\quad A_i\cap A_j=\emptyset(i\neq j)\]

那么有:
\[S(A)=\sum_i P(A_i)[I(A_i)+S(A_i)]=S(\{A_i\})+\sum_i P(A_i)S(A_i)\]

可见把不相交的事件并在一起时,\,总熵就是把\(A_i\)视作元事件的熵加上各个子事件的熵的加权和.

最后让我们来看看作为以上熵的定义所符合的一个十分重要的性质.\,为了方便起见我们把样本空间记做$\Omega$.\,并在样本空间的每个元事件引入两个随机变量$x_i\in X,\,y_j\in Y$.\,还要求两个随机变量独立:
\[P(x_i,y_j)=P(x_i)P(y_j),\,\forall i\forall j\]

那么对于整个样本空间按照随机变量$X$来划分的熵与$Y$的熵:
\[S_X=\sum_i -P(x_i)\ln P(x_i)\quad ;\quad S_Y=\sum_j -P(y_j)\ln P(y_j)\]

而总熵:
\[S=\sum_{ij}-P(x_i,y_j)\ln P(x_i,y_j)\]

计算其关系,\,容易发现:
\begin{align*}
S &=\sum_{ij}-P(x_i,y_j)\ln P(x_i)P(y_j)\\
  &=\sum_{ij}-P(x_i,y_j)\ln P(x_i)+\sum_{ij}-P(x_i,y_j)\ln P(y_j)\\
  &=\sum_{i}\sum_{j}-P(x_i,y_j)\ln P(x_i)+\sum_{j}\sum_{i}-P(x_i,y_j)\ln P(y_j)\\
  &=\sum_i -P(x_i)\ln P(x_i)+\sum_i -P(y_j)\ln P(y_j)\\
  &=S_X+S_Y
\end{align*}

即,\,独立随机变量的熵是可以直接相加的.\,而我们再来分析一下若是非独立随机变量的叠加导致的熵:
\begin{align*}
S_X+S_Y-S &=\sum_{ij}-P(x_i,y_j)\ln P(x_i)+\sum_{ij}-P(x_i,y_j)\ln P(y_j)-\sum_{ij}-P(x_i,y_j)\ln P(x_i,y_j)\\
	      &=\sum_{ij}-P(x_i,y_j)\ln\frac{P(x_i)P(y_j)}{P(x_i,y_j)}\\
	      &\geqslant -\ln\sum_{ij}P(x_i,y_j)\cdot\frac{P(x_i)P(y_j)}{P(x_i,y_j)}\\
	      &=-\ln\sum_{ij}P(x_i)P(y_j)\\
	      &=-\ln 1\\
	      &=0
\end{align*}


其中用到了$\ln$是上凸函数,\,所以自变量的加权平均处的函数值总是大于函数值的加权平均:
\[\ln(\sum_i p_ix_i)\geqslant \sum_ip_i\ln x_i\quad ; \quad \sum_ip_i=1\]

以上结论告诉我们非独立的两个随机变量所带来的熵有一部分是重合的,\,从而导致总的熵小于两个随机变量熵的和.\,小的这一部分叫做两个变量之间的\emph{互信息}(mutual information):
\[M=S_X+S_Y-S\]

互信息代表着已知一个随机变量的结果以后对另一个随机变量条件概率分布的影响.\,例如考虑自然光在某个方向的分振幅,\,以$x$方向的振幅作为第一个随机变量,\,如果另一个方向也是$x$,\,那么互信息恰好等于$x$方向振幅概率分布的熵;\,如果另一个方向为$y$方向,\,那么两个随机变量独立,\,从而互信息为$0$;\,如果选取一个与$x$方向夹锐角的方向,\,则$x$方向的振幅在一定程度上也会影响该方向的振幅的概率分布,\,从而两个随机变量之间具有一定的互信息量,\,并随着夹角增大到$90$度而消失.
\end{comment}

\begin{itemize}
	\item 概率模型:\,\emph{经典概型}(classic models)中原子事件构成的集合是有限的,\,一般为等概率分布.\,也可以想象不是等概率的样本空间.\,事件是部分原子事件构成的子集.\,一般写作:
	\[x\in \Omega,\quad \Omega=\{\omega\}\]
	\[P(A)=P(\bigcup_{\omega_i\in A} {\omega_i})=\sum_{\omega_i\in A} P(\omega_i)\]
	\item 概率模型:\,\emph{几何概型}(geometric models)中的样本空间是与\(\mathbb{R}^n\)具有类似的结构的连续空间,\,一般认为参数化.\,比如原子事件抽象为区间$[a,\,b]$内的一个数,\,或是二维球面$S^2=\{(\theta,\varphi)|\theta\in(0,\,\pi),\,\varphi\in[0,\,2\pi)\}\cup\{\theta=0,\,\theta=\pi\}$上的一个点.\,而概率则是一个归一化的\emph{概率分布函数}(prabability distribution function):
	\[\int_a^b \ud p=\int_a^b p(x)\ud x=1\]
	\[\oint_{S^2} \ud p=\oint_{S^2} p(\theta,\, \varphi)\ud S=1\]

	\item \emph{独立事件}(indepedent events):\,对于经典概型,\,事件$A$与事件$B$独立:
	\[P(A\cap B)=P(A)P(B)\]

	如果把事件的交写作直接的乘积$A\cap B=AB$,\,事件的补$\Omega\backslash A$写作$\bar{A}$,\,上式等价于以下四式:
	\[P(AB)=P(A)P(B)\quad ;\quad P(A\bar{B})=P(A)P(\bar{B})\]
	\[P(\bar{A}B)=P(\bar{A})P(B)\quad ;\quad P(\bar{A}\bar{B})=P(\bar{A})P(\bar{B})\]

	在样本空间定义一个函数变量$X:\,\omega_i\mapsto x_i=X(\omega_i)$,\,那么可以根据不同的函数结果对样本空间进行分类,\,每一个结果就是一个事件,\,概率为$P(x_i)$.\,此时变量$X$与变量$Y$独立即定义为:
	\[\forall x_i\forall y_j:\,P(x_iy_j)=P(x_i)P(y_j)\]

	对于定义在$(x,\,y)\in \mathbb{R}^2$上的几何概型,\,概率分布关于两个坐标独立的条件为可以找到两个坐标各自的概率分布函数:
	\[\forall x\forall y :\,p(x,\,y)=p_x(x)p_y(y)\]

	\item 两种概型下变量$X$的\emph{期望}(expectation)$E=\langle X\rangle $与\emph{方差}(variance)$\sigma^2=\langle (X-E)^2\rangle$的定义:
	\[E=\sum_{\omega_i\in \Omega} P(\omega_i)X(\omega_i)=\int_\Omega X(x)\ud p\]
	\[\sigma^2 =\sum_{\omega_i\in \Omega} P(\omega_i)(X(\omega_i)-E)^2=\int_\Omega (X(x)-E)^2\ud p\]

	\item 两种概型下\emph{统计熵}(statistic entrophy)或\emph{信息熵}(information entropy)的定义:
	\[S=-\sum_{\omega_i\in \Omega} P(\omega_i)\ln P(\omega_i)=-\int_\Omega \ln P(x)\ud p\]

	\item 在$\mathbb{R}$上的分布若要求$\sigma^2$取固定值,\,那么\emph{正态分布}(normal distribution)可以使得熵最大:
	\[p(x)=N(\mu,\,\sigma^2)=\frac{1}{\sqrt{2\pi }\sigma }\ue^{-\frac{-(x-\mu)^2}{2\sigma^2}}\]

	如果还要求期望为零,\,那么上式$\mu=0$.\,注意这等价于说,\,如果一个一维粒子的平均动量为$0$而平均动能为$\langle mv^2/2\rangle=kT/2$,\,熵最大的分布为:
	\[f(v)=\sqrt{\frac{m}{2\pi kT}}\ue^{-\frac{mv^2}{2kT}}\quad ;\quad S=\ln\sqrt{\frac{m}{2\pi kT}}-\frac{1}{2}\]

	\item 如果已知$\mathbb{R}^2$上概率分布$p(x,\,y)$的\emph{边缘分布}(marginal distribution)$p_x,\,p_y$:
	\[p_x(x)=\int_{-\infty}^\infty p(x,\,y)\ud y\]
	\[p_y(y)=\int_{-\infty}^\infty p(x,\,y)\ud x\]

	那么在$\mathbb{R}$分别以$p_x,\,p_y$作为分布函数的分布的熵与二维分布的熵满足:
	\[S\leq S_x+S_y\]

	成立条件是$X$与$Y$随机变量独立:
	\[p(x,\,y)=p_x(x)p_y(y)\]

	这就是说,\,两个随机分布合成时,\,独立合成能够使得熵最大.

\end{itemize}

\section{统计假设}
\begin{comment}
原则上来说,\,经典的物理体系应当是\emph{确定性}(certainty)的,\,用经典力学语言可以描述(质点系的位置与动量)的,\,不包含\emph{随机性}(randomness)的,\,不应引入概率描述的系统.\,然而一方面由于体系在大粒子数大自由度数下所体现出来的统计复杂性,\,即使初态是一个高度特殊的状态:\,比如一个长方体气缸内理想气体初始时刻所有粒子排列为规则的点阵,\,具有完全相同的速度矢量.\,在历经一段时间之后也会体现出完全的随机性:\,体系在很长一段时间内看来各个状态都没有明显的\emph{序}(order),\,显得杂乱无章.\,而稍微有序的状态几乎不可能再次出现.\,这期间的过程与演化,\,原则上说理应是确定性的过程,\,如果我们在某一时刻命所有粒子的速度方向反向,\,这个体系将会重新回到最初的状态.

但是我们将研究\emph{随机过程}(stochastic process),\,在一个随机过程中,\,即使上一刻只有唯一的一种状态,\,下一刻的状态也不是决定性的,\,而是体现出不确定性或是随机性.\,从而对一个时刻系统完整的描述就要包含在各种可能状态下的概率,\,这实际上是一个\emph{系综}(ensemble).\,用系综而不是单个的系统描述,\,从而引入概率的原因有以下几个:

\begin{itemize}
	\item 外在随机性:\,多自由系统是混沌系统,\,初态微小的改变将会随时间以指数形式迅速放大.\,热力学体系总是处在外界环境中,\,外界总是对体系有各种各样的微小扰动.\,即使在之前的情况下在某个时间让所有粒子速度反向,\,也会因为外界对体系的影响而无法观察到体系回到初始状态.
	\item 内在随机性:\,以衰变为例,\,如果认为衰变发生在一个瞬间,\,粒子只可能处于衰变前或衰变后两个不同状态,\,那么衰变就是一个随机的过程.\,每一小段时间粒子均以均等的概率衰变,\,那么大量全同未衰变的粒子构成的体系的未衰变数期望随时间将指数衰减,\,但是每一个时刻未衰变粒子数仍然具有某种概率分布,\,粒子数对于其期望仍具有某种方差.\,事实上原子与原子的碰撞涉及到量子力学行为,\,同样的初始条件是会导致不同的结果的.
\end{itemize}
\end{comment}

\begin{itemize}
	\item \emph{等概率原理}(principle of equal a-priori probability):\,平衡态分布在相空间等能量曲面上所有点处的概率分布函数都应该相等.\,例如大量摆角固定的单摆构成的热力学体系,\,显然摆球出现在最高点的概率要大于最低点的概率(最高点速度慢停的久).\,但是如果把其摆动的$p-x$空间画出来合适的比例下相轨迹为圆,\,而且一定做匀速圆周运动,\,所以肯定单位圆弧上出现摆球的概率相等.\,这个结论是理论力学里通过\emph{刘维尔定理}(Liouville's theorem)的一种合情推理,\,但一般作为平衡态统计力学的最初假设.

	\item 通过等概率原理可以证明,\,如果理想气体也符合这个原理的话,\,平衡态分子速度作为随机变量分布为麦克斯韦分布律.\,这个推导参考热力学统计物理书籍的系综理论.
	\item 通过等概率原理+求\emph{最可几分布}(most probable distribution)也可以得到实际的分布为麦克斯韦分布律.\,这个推导也参考热力学统计物理书籍的最可几分布.
	\item \emph{玻尔兹曼H定理}(Boltzmann H theorem):\,通过\emph{分子混沌假说}(molecular chaos hypothesis),\,认为从非平衡态分子通过碰撞向平衡态转变的过程中,\,每一时刻都存在的概率分布,\,即,\,真实世界的情况只是万千可能情况中的一种.\,而每一次分子的碰撞都是把单一的初态的可能性变成了更多的末态的可能性.\,数学上很容易证明,\,这样的概率演化过程熵只增不减.\,从而末态的熵一定达到了极值.\,再根据上一章的论述,\,粒子在三个方向的速度分布一定是正态分布,\,且彼此独立,\,平均动能一致时熵最大.\,也给出了麦克斯韦分布律.
\end{itemize}


\section{麦克斯韦分布律}

三维麦克斯韦速度分布律是指:
\[f(\bs{v})=(\frac{m}{2\pi kT})^\frac{3}{2}\ue^{-\frac{m\bs{v}^2}{2kT}}=\prod_1^3 \sqrt{\frac{m}{2\pi kT}}\]

\section{能均分定理}

\section{功, 热, 熵}

\section{量子与相对论}