%!TEX root = ../physical-olympics-2.tex
\chapter{统计物理基础}

\section{数学基础}


\subsection{概率与独立性}
现实生活中有很多偶然事件,\,偶然事件的成因是多种多样的,\,它们集中表现在相似的条件下进行试验,\,而能够得到完全不同的结果.\,数学上用抽象的集合来表示所有可能的结果:
\[x\in X,\quad X=\{x\}\]

其中每一个元素代表一种可能的结果.\,这些结果应该有以下特点:
\begin{quote}
{\hei 忠实性}:\,每一个不同的实验结果如实地反应为集合中的不同元素.\,也就是不能用一个元素代替一类实验结果.\\
{\hei 互斥性}:\,当一个结果发生时,\,另一个结果就必须排除,\,也就是不能有多个元素对应同样的实验结果.\\
{\hei 完备性}:\,所有的实验结果必须都有集合中的元素对应.
\end{quote}

这样就能把集合\(X\)称为\emph{样本空间}(sample space),\,而每一个元素\(x\)也称为一个\emph{元事件}(elementary event).\,我们常说的\emph{事件}(event),\,其实一般指样本空间的一些定义良好的子集\(A\subset X\).\,只要实验结果在这个子集中,\,就说这个事件发生了.\,所谓定义良好我们可以做如下理解:

在\emph{古典概型}(classic models)情形下,\,样本空间是一个有限的集合,\,此时任意子集都可以视为某种事件.\,如投一颗骰子,\,样本空间为\(X=\{1,2,3,4,5,6\}\),\,则投出偶数是一个事件\(A=\{2,4,6\}\).

但在\emph{几何概型}(geometric models)情形下,\,样本空间一般具有与\(\mathbb{R}^n\)类似的结构,\,一般都是无限的集合,\,有一些``事件''的提法应当给予摒弃否则会引起矛盾.\,例如在闭区间\([0,1]\)间任取一个点,\,这个点恰好是有理数这样的``事件''可能就不是那么定义良好\footnote{事实上有理数集是定义良好---可测的,\,但有些集合不可测从而不能讨论它们的概率.\,参考\url{https://en.wikipedia.org/wiki/Non-measurable_set}}.\,一般常见的事件如点落在区间\((a,b)\)内等等.

设想同时做好几个实验,\,这几个实验互不干扰\,它们的结果是完全独立的,\,那么联合到一起就构成了一个大的实验,\,其结果应表示为一个数组:
\[\bs{x}=(x_i)\quad ;\quad x_i\in X_i\]

而新的样本空间称为原来那些样本空间的\emph{独立直积}(independent product).\,记做:
\[\bigotimes_i X_i=\{\bs{x}=(x_{ij})|x_{ij}\in X_i\}\]

互斥,\,独立这样的一些概念如何用数学严格表述?\,事实上它们恰恰是用概率去定义的.\,概率是一种我们关于实验结果可能性的\emph{先验假设}(a priori presumption),\,它是\emph{随机变量}(random variable)的正实数值函数\(P\),\,而随机变量既可以取为样本空间中的单个结果,\,也可以取为结果的集合:\,事件:
\[P(A)=P(\bigcup_{x_i\in A} {x_i})=\sum_{x_i\in A} P(x_i)\]

这其中已用到了\(P\)的属性:\,\emph{互斥事件}(mutual-exclusive events, disjoint events)的加法原理:
\[A\cap B=\emptyset\quad \Rightarrow\quad P(A\cup B)=P(A)+P(B)\]

再加上:
\[P(A)\geqslant 0\quad ;\quad P(X)=1\]

就构成了一个合适的概率定义.\,而对于随机变量可取连续样本空间,\,比如区间\([a,b]\)的情形,\,引入\emph{概率密度函数}(probability density function, pdf):
\[P([x,x+\ud x])=p(x)\ud x\quad ;\quad p(x)>0\;,\;\int_a^b p(x)\ud x=1\]

便是一个合适的概率定义.

现在来讨论事件的独立性.\,两个非互斥的事件可能同时发生,\,同时发生这一个新的事件即被定义为\(A\cap B\neq \emptyset\),\,此时可以定义事件\(B\)在事件\(A\)下的\emph{条件概率}(conditional probability):
\[P(B|A)=\frac{P(A\cap B)}{P(A)}\]

显然,\,若条件概率为零,\,那么实际上两个事件互斥.\,而实际上如果\(B\)在\(A\)下的条件概率等于\(B\)的概率,\,那么这种情况称为两个事件相互独立:
\[P(B|A)=P(B)\]

注意到上式实际上也可以写为\emph{独立事件}(independent events)的乘法原理:
\[P(A\cap B)=P(A)P(B)\]

所以两个事件相互独立的确是一个相互的关系,\,此时\(A\)的条件概率也有\(P(A|B)=P(A)\).\,两个事件相互独立是一种很微妙的关系,\,我们注意到如果把事件的交写作直接的乘积$A\cap B=AB$,\,事件的补$X\backslash A$写作$\bar{A}$,\,上式等价于以下四式:
\[P(AB)=P(A)P(B)\quad ;\quad P(A\bar{B})=P(A)P(\bar{B})\]
\[P(\bar{A}B)=P(\bar{A})P(B)\quad ;\quad P(\bar{A}\bar{B})=P(\bar{A})P(\bar{B})\]

这意味着一个事件的发生既不会阻碍另一个事件,\,也不会促成另一个事件.\,而是完全没有影响.\,物理上看,\,很有可能两个事件没有因果关系.

我们之前构造的独立直积,\,用概率表示即为:
\[P((x_{ij}))=\prod_i P(x_{ij})\]

而对于连续概率分布,\,两个样本空间的独立直积给出的新概率应该是有以下概率密度:
\[p(x,y)=p_1(x)p_2(y)\]

\subsection{随机变量及其数字特征}
物理实验中的随机现象,\,常常体现为实验测量中测量结果数据的上下波动.\,此时一般认为这个测量数据\(x\)就是一个随机变量,\,而具有先验的某种概率密度函数\(p(x)\).\,.\,有时候我们关心测量数据的某种函数\(y=f(x)\),\,那么不同的\(y\)其出现概率分布应该修改:
\[p_y(y)=\frac{\ud p}{\ud y}=\frac{p(x)\ud x}{\ud y}=\frac{1}{f'}p(x)\]

例如,\,如果考虑在\(x\)方向的粒子速度\(v\)具有分布律:
\[p(v)=\frac{1}{\sqrt{2\pi}}\ue^{-\frac{v^2}{2}}\]

可以验证这个概率归一,\,那么其能量\(e=\dfrac{v^2}{2}\)的分布为
\[p_{e+}=\frac{p(v)}{v}=\frac{1}{2\sqrt{\pi e}}\ue^{-e}(v>0)\quad ;\quad p_{e-}=\frac{p(v)}{-v}=\frac{1}{2\sqrt{\pi e}}\ue^{-e}(v<0)\]

注意到同一个能量对应着两种可能的运动方向,\,故把两个\(\ud e\)对应概率分开算,\,最后合到一起:
\[p_e(e)=\frac{1}{\sqrt{\pi e}}\ue^{-e}\]

一个在统计中关心的数学处理,\,是在\(n\)次全同独立测量中获得的测量结果的\emph{平均数}(average):
\[\bar{x}=\frac{x_1+x_2+\cdots+x_n}{n}\]

而著名的\emph{大数定律}(law of large numbers)指出:\,如果概率分布的一个重要数字特征:\,\emph{期望}(expectation)存在\footnote{值得指出,\,有些数学上诡异的概率分布是不存在期望的,\,比如典型的柯西分布:
\[p(x)=\frac{\gamma/\pi}{x^2+\gamma^2}\]

当然这样的理想概率分布物理上几乎不存在,\,因为物理量总是有限的,\,在一定尺度下适用.}:
\[\mu=\langle x\rangle=\int x\ud p\]

那么如果随着测量次数趋于无穷,\,平均数将会无限趋近于期望值:
\[\lim_{n\to \infty}P(|\bar{x}-\langle x\rangle|>\varepsilon)=0\; , \; \forall \varepsilon>0\]

如何理解这一点?\,我们再定义另一个随机变量的数字特征:\,\emph{方差}(variance),\,物理上称为\emph{涨落}(fluctuation),\,数学上还称\emph{中心二极矩}(second central moment):
\[\sigma^2=\langle(x-\mu)^2\rangle=\int(x-\int x\ud p)^2\ud p\]

物理上还常常把涨落除以期望的平方称为相对涨落.\,那么可以证明:
\[\langle\bar{x}\rangle=\langle x\rangle\quad ;\quad \langle(\bar{x}-\mu)^2\rangle=\frac{\langle(x-\mu)^2\rangle}{n}\]

故随着实验次数增加,\,平均数作为新的随机变量它的期望不会变,\,而方差在不断减小,\,故有中心极限定理的说法.

以上讨论适用于任何\(y=f(x)\)型的随机变量,\,其中\(x\)可以是一次实验的实数结果,\,也可以是多次实验的数组结果,\,也可以是不同独立甚至非独立测量结果的复合,\,此时\(y\)即被理解为事件,\,代表某些可能发生的元事件的集合.\,期望与方差被定义为:
\[\mu=\int y\ud p\]
\[\sigma^2=\int (y-\mu)^2\ud p\]

其中\(\ud p\)表示\(y\)落在该点附近范围内的概率.

\subsection{信息熵}
为了刻画样本空间与其概率的多样性,\,我们引入\emph{信息熵}(information entropy)的概念.\,对于一个样本空间与随机分布,\,它具有的熵被定义为:
\[S=\sum_{x_i\in X}-P(x_i)\ln P(x_i)\]

而每一种可能性\(x_i\)带来的信息量被定义为\(I_i=-\ln P(x_i)>0\).\,而总熵是事件信息量的期望.\,为什么如此定义?\,比如我们设想投一枚骰子,\,那么投出\(1\)这个数的信息量应该是投出奇数的信息量加上在已知投出奇数后再投出\(1\)的信息量,\,投出奇数称作\(A\)事件那么这个性质写作:
\[I(x)=I(A)+I(x|A)\]

而应该有一种概率与信息量之间的普适对应,\,以上三个信息都可以理解为:
\[I(x)=f(P(x)),\,I(A)=f(P(A)),\,I(x|A)=f(P(x|A))\]

但三个概率本来就有:
\[P(x)=P(A)P(x|A)\]

这暗示着\(f(xy)=f(x)+f(y)\),\,在假定\(f\)性质足够好(可以求二阶导数)情况下:
\[\xrightarrow{\;\partial_y\;} xf'(xy)=f'(y)\]
\[\xrightarrow{\;\partial_x\;} f''(xy)+xyf'(xy)=0\]
\[\xrightarrow{xy=u\;} \frac{\ud}{\ud u}(u\frac{\ud f}{\ud u})=0\]
\[\Rightarrow f=C\ln u+C'\]

重新代入\(f\)性质,\,得\(C'=0\).\,剩下的\(C\)可以任取,\,一般理解为\(-1\),\,这样给出了正的信息量.\,这个推理过程还给出了事件的信息量\(I(A)\)与条件信息量\(I(A|B)\)两个概念的定义.\,根据定义,\,可见\(A\)取整个样本空间的信息量就是零.

现在我们理解到信息量表示稀有程度,\,具体事件发生概率越小则信息量越大.\,那熵又表示什么含义呢.\,它表示多样性.\,我们设想如果某一种元事件\(x_i\)的可能性被发现还可以细分为\(x_{i1},\,x_{i2},\cdots,x_{im}\)种子可能性.\,那么根据此式所引入的熵就会因此而增加,\,增量为:
\begin{align*}
\Delta S & = \sum_{j=1}^m -P(x_{ij})\ln P(x_{ij})-(-P(x_i)\ln P(x_i))\\
		 & = \sum_{j=1}^m -P(x_{ij})\ln P(x_{ij})-\sum_{j=1}^m -P(x_{ij})\ln P(x_i)\\
		 & = \sum_{j=1}^m -P(x_{ij})\ln\frac{P(x_{ij})}{P(x_i)}\\
		 & = \sum_{j=1}^m P(x_{ij})I(x_{ij}|x_i)\\
		 & = P(x_i)\cdot \sum_{j=1}^m -\frac{P(x_{ij})}{P(x_i)}\ln\frac{P(x_{ij})}{P(x_i)}\\
		 & = P(x_i)\cdot \sum_{j=1}^m -P(x_{ij}|x_i)\ln P(x_{ij}|x_i)\\
		 & = P(x_i) S(x_i)
\end{align*}

一方面,\,等于细化这一元事件所增加的信息量的概率平均.\,另一方面,\,也等于事件熵与概率的乘积.\,此处也定义了事件细分熵:
\[S(A)=\sum_{x_i\in A}-P(x_i|A)\ln P(x_i|A)\]

可见单个元事件的细分熵是零.\,而如果把样本空间可以分为一系列不相交的事件:
\[X=\bigcup_i A_i\quad ;\quad A_i\cap A_j=\emptyset(i\neq j)\]

那么有:
\[S(A)=\sum_i P(A_i)[I(A_i)+S(A_i)]=S(\{A_i\})+\sum_i P(A_i)S(A_i)\]

可见把不相交的事件并在一起时,\,总熵就是把\(A_i\)视作元事件的熵加上各个子事件的熵的加权和.

最后让我们来看看作为以上熵的定义所符合的一个十分重要的性质.\,为了方便起见我们把样本空间记做$\Omega$.\,并在样本空间的每个元事件引入两个随机变量$x_i\in X,\,y_j\in Y$.\,还要求两个随机变量独立:
\[P(x_i,y_j)=P(x_i)P(y_j),\,\forall i\forall j\]

那么对于整个样本空间按照随机变量$X$来划分的熵与$Y$的熵:
\[S_X=\sum_i -P(x_i)\ln P(x_i)\quad ;\quad S_Y=\sum_j -P(y_j)\ln P(y_j)\]

而总熵:
\[S=\sum_{ij}-P(x_i,y_j)\ln P(x_i,y_j)\]

计算其关系,\,容易发现:
\begin{align*}
S &=\sum_{ij}-P(x_i,y_j)\ln P(x_i)P(y_j)\\
  &=\sum_{ij}-P(x_i,y_j)\ln P(x_i)+\sum_{ij}-P(x_i,y_j)\ln P(y_j)\\
  &=\sum_{i}\sum_{j}-P(x_i,y_j)\ln P(x_i)+\sum_{j}\sum_{i}-P(x_i,y_j)\ln P(y_j)\\
  &=\sum_i -P(x_i)\ln P(x_i)+\sum_i -P(y_j)\ln P(y_j)\\
  &=S_X+S_Y
\end{align*}

即,\,独立随机变量的熵是可以直接相加的.\,而我们再来分析一下若是非独立随机变量的叠加导致的熵:
\begin{align*}
S_X+S_Y-S &=\sum_{ij}-P(x_i,y_j)\ln P(x_i)+\sum_{ij}-P(x_i,y_j)\ln P(y_j)-\sum_{ij}-P(x_i,y_j)\ln P(x_i,y_j)\\
	      &=\sum_{ij}-P(x_i,y_j)\ln\frac{P(x_i)P(y_j)}{P(x_i,y_j)}\\
	      &\geqslant -\ln\sum_{ij}P(x_i,y_j)\cdot\frac{P(x_i)P(y_j)}{P(x_i,y_j)}\\
	      &=-\ln\sum_{ij}P(x_i)P(y_j)\\
	      &=-\ln 1\\
	      &=0
\end{align*}


其中用到了$\ln$是上凸函数,\,所以自变量的加权平均处的函数值总是大于函数值的加权平均:
\[\ln(\sum_i p_ix_i)\geqslant \sum_ip_i\ln x_i\quad ; \quad \sum_ip_i=1\]

以上结论告诉我们非独立的两个随机变量所带来的熵有一部分是重合的,\,从而导致总的熵小于两个随机变量熵的和.\,小的这一部分叫做两个变量之间的\emph{互信息}(mutual information):
\[M=S_X+S_Y-S\]

互信息代表着已知一个随机变量的结果以后对另一个随机变量条件概率分布的影响.\,例如考虑自然光在某个方向的分振幅,\,以$x$方向的振幅作为第一个随机变量,\,如果另一个方向也是$x$,\,那么互信息恰好等于$x$方向振幅概率分布的熵;\,如果另一个方向为$y$方向,\,那么两个随机变量独立,\,从而互信息为$0$;\,如果选取一个与$x$方向夹锐角的方向,\,则$x$方向的振幅在一定程度上也会影响该方向的振幅的概率分布,\,从而两个随机变量之间具有一定的互信息量,\,并随着夹角增大到$90$度而消失.

我们已经证明, 当已知随机变量$x,\,y$各自满足的分布后, 联合分布$x,\,y$独立时熵最大. 关于信息熵的重要性我们可以再通过以下结论进一步体会:

考虑一维情况下一个粒子速度$v$所具有的分布. 样本空间$X=\mathbb{R}$, 引入粒子的概率分布函数$f(v)$, 粒子速度在$v$到$v+\ud v$之间的概率为:
\[\ud p=f(v)\ud v\]

那么信息熵为:
\[S=-\int f\ln f\ud v\]

我们要求概率归一, 且粒子平衡能量约束为$kT/2$:
\[\int f\ud v=1\]
\[\int \frac{1}{2}mv^2 f\ud v=\frac{1}{2}kT\]

在这样的情况下, 可以通过拉格朗日乘子法求得熵的最大值. 即:
\[-\frac{\partial}{\partial f}(f\ln f)=C_1\cdot \frac{\partial f}{\partial f}+C_2\cdot\frac{1}{2}mv^2\cdot \frac{\partial f}{\partial f}\]

整理可得:
\[f(v)=C\ue^{-\frac{\beta mv^2}{2}}\]

重新代入两个约束条件, 得到:
\[\beta=\frac{1}{kT} \quad,\quad C=\sqrt{\frac{m}{2\pi kT}}\]
\[f(v)=\sqrt{\frac{m}{2\pi kT}}\ue^{-\frac{mv^2}{2kT}}\]

这一种关于速度的分布称作\emph{麦克斯韦分布}(Maxwell distribution). 数学上它属于正态分布. 一般写作:
\[p(x)=N(\mu,\,\sigma^2)=\frac{1}{\sqrt{2\pi }\sigma }\ue^{-\frac{-(x-\mu)^2}{2\sigma^2}}\]

其中$\mu,\,\sigma^2$就是这个分布的期望与方差. 进一步可以得到这个分布的信息熵:
\[S=-\frac{1}{2}\ln{\frac{m}{2\pi kT}}+\frac{1}{2}\]

为何我们如此关心信息熵最大的情况? 联想到热力学中学过的熵增原理, 我们知道从非平衡态到平衡态的自发过程总是使熵最大, 不难推测平衡态熵的确最大. 那么如何从基本原理上论证信息熵的这一特性? 我们需要把这一节的数学知识代入下一节的物理:

\section{玻尔兹曼系综}

\subsection{统计假设}

原则上来说,\,经典的物理体系应当是\emph{确定性}(certainty)的,\,用经典力学语言可以描述(质点系的位置与动量)的,\,不包含\emph{随机性}(randomness)的,\,不应引入概率描述的系统.\,然而一方面由于体系在大粒子数大自由度数下所体现出来的统计复杂性,\,即使初态是一个高度特殊的状态:\,比如一个长方体气缸内理想气体初始时刻所有粒子排列为规则的点阵,\,具有完全相同的速度矢量.\,在历经一段时间之后也会体现出完全的随机性:\,体系在很长一段时间内看来各个状态都没有明显的序,\,显得杂乱无章.\,而稍微有序的状态几乎不可能再次出现.\,这期间的过程与演化,\,原则上说理应是确定性的过程,\,如果我们在某一时刻命所有粒子的速度方向反向,\,这个体系将会重新回到最初的状态.

但是我们将研究\emph{随机过程}(stochastic process),\,在一个随机过程中,\,即使上一刻只有唯一的一种状态,\,下一刻的状态也不是决定性的,\,而是体现出不确定性或是随机性.\,从而对一个时刻系统完整的描述就要包含在各种可能状态下的概率,\,这实际上是一个\emph{系综}(ensemble).\,用系综而不是单个的系统描述,\,从而引入概率的原因有以下几个:

\begin{itemize}
	\item 外在随机性:\,多自由系统是混沌系统,\,初态微小的改变将会随时间以指数形式迅速放大.\,热力学体系总是处在外界环境中,\,外界总是对体系有各种各样的微小扰动.\,即使在之前的情况下在某个时间让所有粒子速度反向,\,也会因为外界对体系的影响而无法观察到体系回到初始状态.
	\item 内在随机性:\,以衰变为例,\,如果认为衰变发生在一个瞬间,\,粒子只可能处于衰变前或衰变后两个不同状态,\,那么衰变就是一个随机的过程.\,每一小段时间粒子均以均等的概率衰变,\,那么大量全同未衰变的粒子构成的体系的未衰变数期望随时间将指数衰减,\,但是每一个时刻未衰变粒子数仍然具有某种概率分布,\,粒子数对于其期望仍具有某种方差.\,事实上原子与原子的碰撞涉及到量子力学行为,\,同样的初始条件是会导致不同的结果的.
\end{itemize}

而系综又可以细分为两种: 首先我们定义, 如果研究的问题中每一个单元(分子或原子等)之间动力学的耦合十分微弱, 远远小于自身动力学量变化和与环境之间的相互作用, 则每一个单元称作系统的\emph{子系}(subsystem), 而它们之间称作\emph{近独立}(nearly indepent)的. 对于近独立子系, 引入\emph{玻尔兹曼系综}(Boltzmann ensemble)是方便的. 它描述了代表性子系各状态的概率分布. 而我们相信粒子数$N$的系统在某种意义上就是这个概率分布独立地重复$N$次.

但是, 这种观点也有它天生的缺陷. 首先如果考虑液体, 固体等相互作用不可忽略的系统从原理上以上方法就无法处理集团行为. 其次, 即使对于近独立子系, 把所有的子系再合到一起, 考虑联合分布也往往是必要的: 这么做能够进一步揭示热力学量与粒子配分之间的紧密联系. 这样我们需要\emph{吉布斯系综}(Gibbs ensemble). 我们将在下一节特别地讨论这个问题. 本节我们只关心近独立子系的玻尔兹曼系综问题.

玻尔兹曼对系统的研究给出了著名的\emph{玻尔兹曼H定理}(Boltzmann H theorem): 通过\emph{分子混沌假说}(molecular chaos hypothesis), 认为从非平衡态分子通过碰撞向平衡态转变的过程中, 每一时刻都存在的概率分布, 即, 真实世界的情况只是万千可能情况中的一种. 而每一次分子的碰撞都是把单一的初态的可能性变成了更多的末态的可能性. 从而玻尔兹曼从数学上证明了, 这样的概率演化过程中一个函数$H$是只减少不增加的:
\[H=\int f\ln f\ud \varLambda \quad,\quad \frac{\ud H}{\ud t}\leq 0\]

其中$\ud \varLambda=\ud v_x\ud v_y\ud v_z$是速度空间的体积元, $f=f(v_x,\,v_y,\,v_z)$是速度空间的分布律. 上式我们可以发现, 玻尔兹曼定义的$H$函数实际上就是信息熵的相反数. 玻尔兹曼H定理, 实际上就是统计的熵增原理. 它的推论为: 平衡态之信息熵应当是最大值.

那么结合上一节论述的内容, 我们就可以说明三个独立的麦克斯韦分布可以使得熵最大, 而且不难说明, 三个分布之平均能量相等时熵是最大的:
\[f(v_x,\,v_y,\,v_z)=f(v_x)f(v_y)f(v_z)=\left(\frac{m}{2\pi kT}\right)^\frac{3}{2}\ue^{-\frac{m\bs{v}^2}{2kT}}\]

但是对于熵我们还需要进行进一步的说明. 热力学熵实际上应当是相空间中的系综熵, 即, 不仅要考虑速度空间中的概率分布, 还需要考虑位形空间中的概率分布(即等概率地均匀分布于体积$V$内). 再考虑$N$个粒子的独立分布, 熵做加法:
\[S=NS_1=N\left(3S_v+S_V\right)\]

而热力学熵实际上还被定义为信息熵的玻尔兹曼常数$k$倍, 根据上一节的计算:
\[S_v=k\cdot \frac{1}{2}\left(1-\ln{\frac{m}{2\pi kT}}\right)\]
\[S_V=-k\int \frac{1}{V}\ln \frac{1}{V}\ud V=k\ln V\]

代入得到:
\[S=\frac{3}{2}Nk\ln T+Nk\ln V+\frac{Nk}{2}\left(1-\ln{\frac{m}{2\pi k}}\right)\]

可见其于单原子分子理想气体的熵的确是符合的. 我们利用统计方法给出了除了含$T,\,V$项之外的其他项. 但是我们遇到了一个问题: 似乎以上熵的表达式不符合广延性? 事实上, 这和气体混合带来的混合熵的道理是相同的. 将$N$个粒子的熵直接相加, 实际上暗示了每一个粒子都是被独立描述的. 但是实际上, 我们不需要描述每一个粒子在何种状态, 只需要描述相空间中点的分布. 这一项修正往往可以通过将样本空间的体积元相乘以后除以$N!$来修正. 从而正确的熵公式为:
\[S=\frac{3}{2}Nk\ln T+Nk\ln V+\frac{Nk}{2}\left(1-\ln{\frac{m}{2\pi k}}\right)-k\ln N!\]

利用著名的\emph{斯特令公式}(Stirling's formula):
\[\ln N!\approx N\ln N-N\]

得到:
\[S=\frac{3}{2}Nk\ln T+Nk\ln \frac{V}{N}+\frac{Nk}{2}\left(3-\ln{\frac{m}{2\pi k}}\right)\]

这就是广延性的熵公式.

还有一个问题: 对于多原子分子, 如何理解其熵的区别? 事实上多出来的熵项来自于分子内禀自由度上概率分布产生的熵项. 如何从原理上论述信息熵在这种情况下依然要取最大值? 这种情况下普遍的证明需要依据一般的吉布斯系综理论或最可几分布理论. 我们在此仅做类比推广: 

考虑任意广义坐标$q$和广义动量$p$构成的相空间, 热平衡时子系符合\emph{玻尔兹曼分布律}(Boltzmann distribution law): 若粒子在某一自由度上的能量(哈密顿量)为$h(q,\,p)$, 那么概率即为:
\[\ud p=C\ue^{-\frac{h(q,\,p)}{kT}}\ud p\ud q\]

其中$C$为归一化常数, 可积分计算:
\[C=\left[\iint \ue^{-\frac{h(q,\,p)}{kT}}\ud p\ud q\right]^{-1}\]

那么该自由度上的平均能量为:
\[u=\frac{\iint h(q,\,p)\ue^{-\frac{h(q,\,p)}{kT}}\ud p\ud q}{\iint \ue^{-\frac{h(q,\,p)}{kT}}\ud p\ud q}\]

特别地, 我们有能均分定理: 每一个平方项形式的能量在玻尔兹曼分布律下都具有如下相同的平均能量:
\[\overline{\frac{1}{2}M\dot{q}^2}=\overline{\frac{1}{2}Kq^2}=\frac{1}{2}kT\]

\subsection{麦克斯韦分布律}

三维\emph{麦克斯韦速度分布律}(Maxwell distribution law)是指:
\[f(\bs{v})=\left(\frac{m}{2\pi kT}\right)^\frac{3}{2}\ue^{-\frac{m\bs{v}^2}{2kT}}=\prod_1^3 \sqrt{\frac{m}{2\pi kT}}\ue^{-\frac{mv_i^2}{2kT}}\]

\begin{itemize}
\item 这是速度空间中的概率分布函数. 即若分子数$N$很大则$\ud N = Nf(\bs{v})\ud v_x\ud v_y\ud v_z$.

\item 由于三个方向分布律独立, 每个方向上的分布律也叫麦克斯韦速度分布律:
\[f(v_i)=\sqrt{\frac{m}{2\pi kT}}\ue^{-\frac{mv_i^2}{2kT}}\]
\[f(\bs{v})=f(v_x)f(v_y)f(v_z)\]

\item 麦克斯韦速率分布律是指速度空间中等速率球之间的概率微元决定的函数:
\[\frac{\ud N}{N}=\ud p=F(v)\ud v \]
\[F(v)=4\pi v^2\left(\frac{m}{2\pi kT}\right)^{\frac{3}{2}}\ue^{-\frac{mv^2}{2kT}}\]

\item 平均速率与方均根速率:
\[\overline{v}=\sqrt{\frac{8kT}{\pi m}}\quad ;\quad \sqrt{\overline{v^2}}=\sqrt{\frac{3kT}{m}}\]

\item 泻流数与压强既可以用速率分布计算也可以用单方向的分布函数计算:
\[\varGamma=\frac{1}{4}n\overline{v}\quad ;\quad \ud \varGamma=\frac{1}{4}\ud nv\]
\[p=\frac{2}{3}n\overline{\varepsilon}\quad ;\quad \ud p=\frac{2}{3}\ud n\varepsilon\]

\item 光子气的以上公式是类似的:
\[\varGamma=\frac{1}{4}nc\quad ;\quad \ud \varGamma=\frac{1}{4}\ud nc\]
\[p=\frac{1}{3}n\overline{\varepsilon}\quad ;\quad \ud p=\frac{1}{3}\ud n\varepsilon\]
\end{itemize}



\section{吉布斯系综}

本节我们依然只研究近独立子系问题. 将玻尔兹曼系综推广为吉布斯系综的必要性是显而易见的. 我们思考: 每一个粒子在其相空间中的概率分布真的是独立的吗? 比如在平均能量附近取一个速度区间$\ud v$使$\ud p=f(v)\ud v$与非平均能量处的另一个区间内等概率$\ud p=f(v')\ud v'$, 如果概率分布独立, 那么$N$个粒子速度均落在两个区间内也应该是等概率的. 但这显然是荒谬的: 因为$v\neq v'$意味着后者会使得整个系统能量显著地偏离热平衡值, 且不说孤立系统情况后者根本就不可能发生, 即使系统与外界处于热平衡下, 考虑系统可能的能量涨落, 这么大的涨落的可能性也是微乎其微的.

从而考虑$N$个粒子的联合概率分布一定是必要的, 如果有的粒子能量较高, 那么剩下的粒子在能量较低的状态上概率就会升高. 如何处理这样的问题? 一种可用思路就是纯粹系综的思想: 我们可以根据子系(自由度为$f$)相空间维数$2f$构造$2Nf$维的完整相空间考虑其上的概率分布. 另一种思路就是构造\emph{最可几分布}(most probable distribution)方法. 为了方便说明这个过程, 我们需要先将状态量子化:

\subsection{再论统计假设}

我们知道, 粒子具有波动性, 利用波动来理解粒子的运动状态将会发现, 粒子的能量或状态都是量子化的. 它们由一系列量子数来描述, 我们取为:
\[l=(0,\,)1,\,2,\,3\cdots\]

它们对应能级:
\[(\varepsilon_0<)\varepsilon_1<\varepsilon_2<\varepsilon_3<\cdots\]

其中$\varepsilon_0$或$\varepsilon_1$称作基态能量. 还需要注意, 能级与状态并不是一一对应. 我们将$\varepsilon_l$对应的状态数称作\emph{简并度}(degeneracy)$g_l$. 例如, 我们熟悉的三维氢原子模型\footnote{有心的同学可能会发现, 氢原子体系的热平衡按本节提供的热学方法计算会得到荒谬的发散结果. 这是因为$l$很大的情况下会由于电子波函数的非定域性不再可以忽略原子数密度带来的限制, 从而$l$可视作存在截断上限.}符合:
\[\varepsilon_l=-\frac{13.6{\rm eV}}{l^2} \quad,\quad g_l=2l^2\]

我们另外举三个例子, 它们的证明需要依赖量子力学计算. 一是一维限制于长度为$L$的容器中的质量为$m$的单原子分子, 其平动能级\footnote{请注意, $l=0$的状态量子力学不允许存在, 但是如果是一维圆环上的原子则允许存在.}:
\[l\geq 1 \quad:\quad \varepsilon_l=l^2\frac{\pi^2\hbar^2}{2mL^2} \quad,\quad g_l=2\]

二是双原子分子的转动, 折合质量$\mu$, 分子间距$R$时. 若定义转动惯量为$I=\mu R^2$, 则转动能级:
\[l\geq 1 \quad:\quad \varepsilon_l=l(l+1)\frac{\hbar^2}{2I} \quad,\quad g_l=2l+1\]

三是双原子分子的振动, 惯性系数为$M$, 角频率为$\omega$. 其振动能级:
\[l\geq 0 \quad:\quad \varepsilon_l=\left(l+\frac{1}{2}\right)\hbar \omega \quad,\quad g_l=1\]

从而无论任何情况下, 我们都可以如此理解粒子的熵: 首先是考虑每一个粒子在体积$V$内的均匀分布, 要注意这些分布的确是独立的(而不像能量那样总量固定!), 再考虑粒子的全同性, 根据上一节, 这一步带来熵:
\[S_V=Nk\ln V-k\ln N!=Nk\ln\frac{V}{N}+Nk\]

接下来只需要考虑确定了每一个粒子的位置以后, 按位置编号$1,\,2,\,3\cdot N$号粒子各处于什么状态, 这种情况称作\emph{定域子系}(localized subsystem). 我们称为每一个粒子指定一个能级$\varepsilon_l$中的$g_l$个状态中的一种为一个\emph{微观态}(microstate). 很明显, 微观态的总的个数为:
\[\sum W=W_0=\left(\sum_l g_l\right)^N\]

假设经过统计以后, 处于$\varepsilon_l$状态的粒子数为$N_l$. 那么把为每一个$\varepsilon_l$指定一个$N_l$为一个\emph{宏观态}(macrostate). 宏观态本质上就是对于内能$U$的\emph{配分}(partition), 我们考虑孤立系统, 内能总量固定. 从而配分$N_l$需要符合:
\[\sum_l N_l=N\]
\[\sum_l N_l\varepsilon_l=U\]

每一个宏观态实际上由大量微观态组成. 配分$N_l$所对应的微观态的个数应当是:
\[W=\frac{N!}{\displaystyle\prod_l N_l!}g_l^{N_l}\]

所谓最可几方法实际上就是要研究, 在何种配分$N_l$下$W$取极值. 这是因为\footnote{实际上这个简陋的方法问题太多了: \begin{enumerate}
	\item 量子力学指出, 粒子可以处于多个状态的叠加态而不总能指定单状态.
	\item 定域假设不总是能成立, 我们预先假定位于空间各处的粒子可能因为波函数交叠而无法判断处在某个态的是哪边的粒子.
	\item 事实上量子力学可以证明仅当$N_l/g_l\ll 1$时定域假设成立. 但此时尤其是当$g_l=1$时, $N_l$不是整数不可以计算阶乘, 或者阶乘无法用斯特令公式进行近似.
	\item $W_0$个微观态中, 不仅是符合$N,\,U$给定的那些微观态是需要考虑的, 实际还可以考虑与外界相互作用导致的$N,\,U$涨落而放宽以上两个配分等式的要求. 称作\emph{正则系综}(canonical ensemble)和\emph{巨正则系综}(grand canonical ensemble). 我们仅考虑了$N,\,U$固定的\emph{微正则系综}(micro-canonical ensemble).
\end{enumerate}
}\emph{等概率原理}(principle of equal a-priori probability): 第一, 我们认为在$W_0$个微观态中, 只有配分符合能量之和等于$U$的那些微观态才有概率出现. 即只需要考虑``等能量曲面'', 其微观态总数为$W_U$. 第二, 这个``等能量曲面''上每一个微观态具有相等的概率$1/W_U$, 这是为了使得熵最大. 此时这个分布的熵为$S_U=k\ln W_U$. 但是考虑配分$N_l$的宏观态, 如果其微观态个数为$W$, 其概率为:
\[p=\frac{W}{W_U}\]

从而$W$最大时出现概率最大, 把这个宏观态的配分作为平衡态下的平均配分, 就可以得出宏观热力学量的特性. 这就是最可几方法的精神. 而且可以进一步, 如果热力学体系初始处于非平衡的配分, 那么向平衡态的平均配分的转变就是一个熵增的过程. 基于这一点, 定义配分$N_l$对应的熵:
\[S_P=k\ln W\]

求$W$的最大值, 实际上也就是求$S_P$的最大值. 本节的吉布斯系综最可几分布方法就与上一节的玻尔兹曼系综方法统一了.

将配分的微观态数代入配分熵, 并利用斯特令公式做近似化简得到:
\[S_P=-k\sum_l N_l\ln\frac{N_l}{Ng_l}\]

同样用拉格朗日乘子法求$S_P$极值. 得到:
\[N_l=g_l\ue^{-\alpha-\beta \varepsilon_l}\]

这实际上就是玻尔兹曼分布. 对于麦克斯韦分布可以发现:
\[\beta=\frac{1}{kT}\]

我们定义系统的配分函数为:
\[Z(\beta,\,x)=\sum_l g_l\ue^{-\beta \varepsilon_l}\]

其中$x$为$\varepsilon_l$中可能含有的广义坐标. 例如一维箱子的长度$L$. 那么我们通过归一化条件可以给出$\alpha$:
\[\ue^{-\alpha}=\frac{N}{Z} \quad,\quad \alpha=-\ln\frac{N}{Z}\]

从而温度为$T$, 广义坐标为$x$时平衡态下的配分为:
\[N_l=N\frac{\ue^{-\beta \varepsilon_l}}{Z(\beta,\,x)}\]



\subsection{功, 热, 熵}

\begin{itemize}
	\item 通过以上近独立子系的分析可以发现, 体系的内能为温度和哈密顿量中参数(比如系统的体积等整体广义坐标)的函数:
	\[U=Nu=U(T,\,Q_\alpha)\]
	\[u=\iint h(q,\,p,\,Q_\alpha)\rho (q,\,p,\,Q_\alpha,\,T)\ud p\ud q\]
	\[\rho (q,\,p,\,Q_\alpha,\,T)=\left.\ue^{-\frac{h(q,\,p,\,Q_\alpha)}{kT}}\middle/\iint \ue^{-\frac{h(q,\,p,\,Q_\alpha)}{kT}}\ud p\ud q\right.\]
	\item 绝热功: 在一个过程中, 哈密顿量中参数改变, 温度改变, 导致分布不变:
	\[\ud\left(\frac{h(q,\,p,\,Q_\alpha)}{kT}\right)=0\quad \Rightarrow \quad\ud \rho (q,\,p,\,Q_\alpha,\,T)=0\]
	\[\ud u|_{\rm diabatic}=\iint \ud h(q,\,p,\,Q_\alpha)\rho (q,\,p,\,Q_\alpha,\,T)\ud p\ud q\]
	\[\ud U|_{\rm diabatic}=\sum Y_\alpha \ud Q_\alpha\]

	\item 热: 内能改变的其他部分:
	\[\ud q=\ud u-\ud u|_{\ud Q=0}=\iint h(q,\,p,\,Q_\alpha)\ud \rho (q,\,p,\,Q_\alpha,\,T)\ud p\ud q\]

	\item 熵: 容易证明, 如果定义:
	\[s=\frac{u}{T}+k\ln \iint\ue^{-\frac{h(q,\,p,\,Q_\alpha)}{kT}}\ud p\ud q\]

	则有:
	\[\ud q=T\ud s\]
\end{itemize}

